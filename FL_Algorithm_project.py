# -*- coding: utf-8 -*-
"""COMP5434 Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cWCBYixtyQ5QkW6bZfFqwQ3fipBywffg
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Define the number of clients
num_clients = 4

# Load the data
train_data = pd.read_csv('Train_Data.csv')

# Convert date to datetime64 and integer
train_data['date'] = pd.to_datetime(train_data['date'])
train_data['date'] = train_data['date'].astype(np.int64)

# Convert categorical columns to numerical codes
cat_cols = ['district', 'city', 'zip code', 'region']
for col in cat_cols:
    train_data[col] = pd.factorize(train_data[col])[0]

# Convert total cost variable to categorical
train_data['total cost'] = pd.Categorical(train_data['total cost'])
train_data['total cost'] = train_data['total cost'].cat.codes
train_data['total cost'] = tf.keras.utils.to_categorical(train_data['total cost'], num_classes=2)

# Split the data into four parts
np.random.seed(42)
train_data = train_data.sample(frac=1)
data_splits = np.array_split(train_data, num_clients)

# Define the model architecture
model = tf.keras.Sequential([
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(16, activation='relu'),
    tf.keras.layers.Dense(1)
])

# Define the number of epochs and batch size for each client
num_epochs = 10
batch_size = 38

# Load the test data
test_data = pd.read_csv('Test_Data.csv')

# Convert date to datetime64 and integer
test_data['date'] = pd.to_datetime(test_data['date'])
test_data['date'] = test_data['date'].astype(np.int64)

# Convert categorical columns to numerical codes
cat_cols = ['district', 'city', 'zip code', 'region']
for col in cat_cols:
  test_data[col] = pd.factorize(test_data[col])[0]
# Define the test dataset
test_features = test_data.iloc[:, :-1].values.astype(np.float32)
test_labels = test_data.iloc[:, -1].values.astype(np.float32)
test_ds = tf.data.Dataset.from_tensor_slices((test_features, test_labels)).batch(batch_size)
#填充测试集
train_cols = train_data.columns
test_data = test_data.reindex(columns=train_cols, fill_value=0)
# Compute mean and standard deviation of numerical columns in training set
num_cols = train_data.select_dtypes(include=[np.number]).columns.tolist()
mean = train_data[num_cols].mean()
std = train_data[num_cols].std()

# Normalize numerical columns in test set using mean and std of training set
test_data[num_cols] = (test_data[num_cols] - mean) / std

# Define the optimizer and loss function
optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
loss_fn = tf.keras.losses.MeanSquaredError()

# Compile the model
model.compile(optimizer=optimizer, loss=loss_fn, metrics=['mae'])

# Train the model using Federated Learning
def federated_train(model, data_splits, num_epochs, batch_size, test_data):

  for i in range(len(data_splits)):
    # Split the data into training and validation sets for this client
    train_data, val_data = train_test_split(data_splits[i], test_size=0.2, random_state=42)

    # Normalize numerical columns in training data
    train_data.loc[:,num_cols] = (train_data.loc[:,num_cols] - mean) / std

    # Normalize numerical columns in validation data
    val_data.loc[:,num_cols] = (val_data.loc[:,num_cols] - mean) / std

    # Define the client's dataset
    train_features = train_data.iloc[:, :-1].values.astype(np.float32)
    train_labels = train_data.iloc[:, -1].values.astype(np.float32)
    train_ds = tf.data.Dataset.from_tensor_slices((train_features, train_labels)).shuffle(buffer_size=len(train_data)).batch(batch_size)

    # Define the validation dataset
    val_features = val_data.iloc[:, :-1].values.astype(np.float32)
    val_labels = val_data.iloc[:, -1].values.astype(np.float32)
    val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_labels)).batch(batch_size)

    # Train the model on the client's dataset
    best_val_loss = float('inf')
    for epoch in range(num_epochs):
      for x_batch_train, y_batch_train in train_ds:
        with tf.GradientTape() as tape:
            y_pred = model(x_batch_train, training=True)
            loss = loss_fn(y_batch_train, y_pred)
        grads = tape.gradient(loss, model.trainable_weights)
        optimizer.apply_gradients(zip(grads, model.trainable_weights))

      # Evaluate the client's model on the validation and test sets
      _, val_mae = model.evaluate(val_ds)
      _, test_mae = model.evaluate(test_ds)

      if val_mae < best_val_loss:
        best_val_loss = val_mae
        best_weights = model.get_weights()

    print(f'Client {i+1} - validation MAE: {val_mae:.2f}, test MAE: {test_mae:.2f}')

    # Set the model weights to the best weights found on the validation set
    model.set_weights(best_weights)
  return model
federated_model = federated_train(model, data_splits, num_epochs, batch_size, test_data)

# Train the federated model
for i in range(num_clients):
    best_weights = None  # reset best_weights for each client
    federated_model = federated_train(model, data_splits, num_epochs, batch_size, test_data)
    client_weight = 1.0 / num_clients
    model_weight = (1.0 - client_weight)
    federated_weights = federated_model.get_weights()
    current_weights = model.get_weights()
    new_weights = []
    for i in range(len(federated_weights)):
        new_weights.append((client_weight * federated_weights[i]) + (model_weight * current_weights[i]))
    model.set_weights(new_weights)

_, test_mae = model.evaluate(test_ds)

print('Test MAE: {:.2f}'.format(test_mae))

"""
        # Train the model on the client's dataset
        for epoch in range(num_epochs):
          for x_batch_train, y_batch_train in train_ds:
            x_batch_train = np.array(x_batch_train, dtype=np.float32)
            y_batch_train = np.array(y_batch_train, dtype=np.float32)
            with tf.GradientTape() as tape:
              y_pred = model(x_batch_train, training=True)
              loss = loss_fn(y_batch_train, y_pred)
            grads = tape.gradient(loss, model.trainable_weights)
            optimizer.apply_gradients(zip(grads, model.trainable_weights))
        
        # Split the validation set into features and labels
        val_features = val_data.iloc[:, : ].values.astype(np.float32)
        val_labels = val_data.iloc[:, ].values.astype(np.float32)"""

print(train_data.describe())

# Load the test data
test_data = pd.read_csv('Test_Data.csv')

# Convert date to datetime64 and integer
test_data['date'] = pd.to_datetime(test_data['date'])
test_data['date'] = test_data['date'].astype(np.int64)

# Make predictions using the FL model
test_ds = tf.data.Dataset.from_tensor_slices(test_data.values.astype(np.float32))
test_ds = test_ds.batch(batch_size)
y_pred_fl = federated_model.predict(test_ds)

# Add the predictions to the test data
test_data['predictions'] = y_pred_fl

# Save the updated test data to a CSV file
test_data.to_csv('Test_Data.csv', index=False)